{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6212f64",
   "metadata": {},
   "source": [
    "#  Data Preprocessing Pipeline (NumPy Only)\n",
    "\n",
    "## 1. Overview\n",
    "This notebook executes the data preprocessing pipeline. To ensure **Data Consistency** and prevent **Data Leakage**, we adhere to the following strict protocol:\n",
    "\n",
    "1.  **Fit on Training Set:** Calculate statistics (Mean, Median, Mode, Categories) solely based on `aug_train.csv`.\n",
    "2.  **Transform on Test Set:** Apply the *same* statistics and mappings derived from the Training set to `aug_test.csv`.\n",
    "\n",
    "**Key Steps:**\n",
    "* **Cleaning:** Handle specific string values (`<1`, `>20`, `never`).\n",
    "* **Imputation:** Fill missing values using Median (Numeric) or \"Unknown\"/Mode (Categorical).\n",
    "* **Feature Engineering:** Log-transform `training_hours`.\n",
    "* **Encoding:** Ordinal Encoding for ranked data, One-Hot Encoding for nominal data.\n",
    "* **Scaling:** Z-score Standardization using Training set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24cff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and custom modules loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import custom functions from src\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.data_processing import (\n",
    "    load_data, \n",
    "    convert_experience, \n",
    "    convert_last_new_job, \n",
    "    ordinal_encode, \n",
    "    one_hot_encode\n",
    ")\n",
    "\n",
    "print(\"Libraries and custom modules loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2bdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Data loaded successfully. \n",
      "Data loaded successfully. \n",
      "Original Train shape: (19158,)\n",
      "Original Test shape : (2129,)\n",
      "Test IDs saved to ../data/processed/test_ids.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. LOAD DATA & SETUP ---\n",
    "print(\"Loading raw data...\")\n",
    "train_data = load_data('../data/raw/aug_train.csv')\n",
    "test_data = load_data('../data/raw/aug_test.csv')\n",
    "\n",
    "print(f\"Original Train shape: {train_data.shape}\")\n",
    "print(f\"Original Test shape : {test_data.shape}\")\n",
    "\n",
    "# 2. Separate Target (Only in Train)\n",
    "y_train_raw = train_data['target'].astype(int)\n",
    "\n",
    "# 3. Save IDs for Submission to CSV (NumPy Only)\n",
    "# Lưu lại ID của tập Test để sau này ghép vào file nộp bài\n",
    "test_ids = test_data['enrollee_id']\n",
    "\n",
    "# Tạo thư mục nếu chưa có\n",
    "if not os.path.exists('../data/processed/'):\n",
    "    os.makedirs('../data/processed/')\n",
    "\n",
    "# Lưu thành file CSV (Dạng số nguyên %d)\n",
    "np.savetxt(\n",
    "    '../data/processed/test_ids.csv', \n",
    "    test_ids,\n",
    "    delimiter=\",\",\n",
    "    header=\"enrollee_id\",\n",
    "    comments='',  \n",
    "    fmt='%d'      \n",
    ")\n",
    "\n",
    "print(\"Test IDs saved to ../data/processed/test_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841fae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_pipeline(data, is_train=True, stats=None):\n",
    "    \"\"\"\n",
    "    Hàm xử lý dữ liệu trung tâm.\n",
    "    - is_train=True: Tính toán Median/Mode/Categories và lưu vào 'stats'.\n",
    "    - is_train=False: Dùng 'stats' để xử lý (không tính lại).\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Nếu là Train, khởi tạo stats mới. Nếu là Test, phải cung cấp stats.\n",
    "    if is_train:\n",
    "        stats = {}\n",
    "    elif stats is None:\n",
    "        raise ValueError(\"Must provide 'stats' when processing Test data!\")\n",
    "\n",
    "    # --- 1. NUMERICAL FEATURES ---\n",
    "    \n",
    "    # A. City Development Index (Giữ nguyên)\n",
    "    cdi = data['city_development_index']\n",
    "    X_list.append(cdi.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('city_dev_index')\n",
    "\n",
    "    # B. Training Hours (Log Transform)\n",
    "    hours = np.log1p(data['training_hours'].astype(float))\n",
    "    X_list.append(hours.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('log_training_hours')\n",
    "\n",
    "    # C. Experience (Convert + Fill Median)\n",
    "    exp = convert_experience(data['experience'])\n",
    "    if is_train:\n",
    "        # Tính Median chỉ trên tập Train\n",
    "        stats['exp_median'] = np.nanmedian(exp)\n",
    "    # Fill bằng Median đã lưu\n",
    "    exp[np.isnan(exp)] = stats['exp_median']\n",
    "    X_list.append(exp.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('experience_years')\n",
    "\n",
    "    # D. Last New Job (Convert + Fill 0)\n",
    "    lnj = convert_last_new_job(data['last_new_job'])\n",
    "    lnj[np.isnan(lnj)] = 0\n",
    "    X_list.append(lnj.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('last_new_job_years')\n",
    "\n",
    "    # --- 2. ORDINAL FEATURES (Ranking) ---\n",
    "    \n",
    "    # A. Education Level\n",
    "    edu_map = {'Primary School': 0, 'High School': 1, 'Graduate': 2, 'Masters': 3, 'Phd': 4, '': 2, 'nan': 2}\n",
    "    edu_encoded = ordinal_encode(data['education_level'].astype(str), edu_map)\n",
    "    X_list.append(edu_encoded.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('education_level_ord')\n",
    "\n",
    "    # B. Company Size\n",
    "    size_map = {\n",
    "        '<10': 0, '10/49': 1, '50-99': 2, '100-500': 3, \n",
    "        '500-999': 4, '1000-4999': 5, '5000-9999': 6, '10000+': 7, \n",
    "        '': -1, 'nan': -1\n",
    "    }\n",
    "    size_encoded = ordinal_encode(data['company_size'], size_map)\n",
    "    X_list.append(size_encoded.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('company_size_ord')\n",
    "\n",
    "    # C. Enrolled University\n",
    "    univ_map = {'no_enrollment': 0, 'Part time course': 1, 'Full time course': 2, '': 0, 'nan': 0}\n",
    "    univ_encoded = ordinal_encode(data['enrolled_university'], univ_map)\n",
    "    X_list.append(univ_encoded.reshape(-1, 1))\n",
    "    if is_train: feature_names.append('enrolled_university_ord')\n",
    "\n",
    "    # --- 3. NOMINAL FEATURES (One-Hot) ---\n",
    "    # Test set bắt buộc phải dùng đúng danh sách cột (categories) của Train set\n",
    "    \n",
    "    nominal_cols = ['gender', 'company_type', 'major_discipline']\n",
    "    fill_vals = ['Unknown', 'Unknown', 'STEM'] # Giá trị fill mặc định\n",
    "    \n",
    "    for i, col_name in enumerate(nominal_cols):\n",
    "        raw_col = data[col_name].astype(str)\n",
    "        # 1. Fill missing cơ bản\n",
    "        raw_col[(raw_col == '') | (raw_col == 'nan')] = fill_vals[i]\n",
    "        \n",
    "        # 2. Encoding logic\n",
    "        if is_train:\n",
    "            # FIT: Tìm unique categories và lưu vào stats\n",
    "            oh_matrix, cats = one_hot_encode(raw_col)\n",
    "            stats[f'cat_{col_name}'] = cats # Lưu lại danh mục\n",
    "            feature_names.extend([f\"{col_name}_{c}\" for c in cats])\n",
    "        else:\n",
    "            # TRANSFORM: Lấy categories cũ ra dùng\n",
    "            saved_cats = stats[f'cat_{col_name}']\n",
    "            oh_matrix = (raw_col[:, None] == saved_cats[None, :]).astype(int)\n",
    "            \n",
    "        X_list.append(oh_matrix)\n",
    "\n",
    "    # Tổng hợp thành ma trận\n",
    "    X_final = np.column_stack(X_list).astype(float)\n",
    "    \n",
    "    return X_final, stats, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f5e9e",
   "metadata": {},
   "source": [
    "###  The Core Processing Logic: `process_data_pipeline`\n",
    "\n",
    "The function below serves as the \"heart\" of our preprocessing workflow. It is explicitly designed to ensure **Data Consistency** between the Training and Test sets while strictly adhering to the **\"Fit on Train, Transform on Test\"** principle.\n",
    "\n",
    "####  Key Objectives:\n",
    "1.  **Prevent Data Leakage:** We strictly avoid calculating any statistics (Mean, Median, Categories) on the Test set. All parameters are learned solely from the Training set.\n",
    "2.  **Data Consistency:** We ensure that the processed Test set has exactly the **same number of columns** and the **same column order** as the Training set (crucial for One-Hot Encoding).\n",
    "\n",
    "####  Mechanism:\n",
    "The function accepts an `is_train` flag and a `stats` dictionary:\n",
    "\n",
    "* **FIT Phase (`is_train=True`):**\n",
    "    * Calculates the **Median** for numerical columns (e.g., `experience`).\n",
    "    * Identifies unique **Categories** for nominal columns.\n",
    "    * Saves all these parameters into the `stats` dictionary and returns it for future use.\n",
    "\n",
    "* **TRANSFORM Phase (`is_train=False`):**\n",
    "    * Does **not** recalculate anything.\n",
    "    * Uses the Median from `stats` to fill missing values.\n",
    "    * Uses the Category list from `stats` to generate One-Hot columns (ensuring the Test set structure matches the Train set, regardless of missing categories in the Test data).\n",
    "\n",
    "####  Technical Details per Group:\n",
    "1.  **Numerical Features:**\n",
    "    * `training_hours`: Applied **Log Transform** (`np.log1p`) to handle skewed distribution.\n",
    "    * `experience`: Missing values filled using the **Median** (learned from Train).\n",
    "\n",
    "2.  **Ordinal Features (Hierarchical Mapping):**\n",
    "    * Mapped strings to integers based on a predefined hierarchy to preserve rank information.\n",
    "    * **`education_level`**: Converted to a **0-4 scale**:\n",
    "        * `Primary School`: 0 $\\rightarrow$ `High School`: 1 $\\rightarrow$ `Graduate`: 2 $\\rightarrow$ `Masters`: 3 $\\rightarrow$ `Phd`: 4.\n",
    "    * **`company_size`**: Converted to a **0-7 scale**:\n",
    "        * `<10`: 0 $\\rightarrow$ `10/49`: 1 $\\rightarrow$ ... $\\rightarrow$ `10000+`: 7.\n",
    "        * Missing values (`nan` / `''`) are mapped to **-1**.\n",
    "    * **`enrolled_university`**: Converted to a **0-2 scale**:\n",
    "        * `no_enrollment`: 0 $\\rightarrow$ `Part time course`: 1 $\\rightarrow$ `Full time course`: 2.\n",
    "\n",
    "3.  **Nominal Features (One-Hot Encoding):**\n",
    "    * Utilizes NumPy **Broadcasting**: `(col[:, None] == saved_cats[None, :])`.\n",
    "    * Categorical variables are expanded into binary matrixes (0/1), where the number of new columns equals the number of unique categories found in the Training set:\n",
    "        * **`gender`**: Expands into **4 columns** (`Female`, `Male`, `Other`, `Unknown`).\n",
    "        * **`company_type`**: Expands into **7 columns** (`Pvt Ltd`, `Funded Startup`, `Early Stage Startup`, `NGO`, `Public Sector`, `Other`, `Unknown`).\n",
    "        * **`major_discipline`**: Expands into **6 columns** (`STEM`, `Humanities`, `Business Degree`, `Arts`, `No Major`, `Other`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356233f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 1] PROCESSING TRAINING DATA\n",
      "   Input Shape: (19158,)\n",
      "   >> Processing Complete.\n",
      "   Output Shape: (19158, 24) (24 features generated)\n",
      "   >> LEARNED STATS (Evidence):\n",
      "      - Median Experience (imputed): 9.0 years\n",
      "      - Gender Categories found    : 4 ['Female' 'Male' 'Other' 'Unknow']\n",
      "      - Company Types found        : 7\n",
      "\n",
      "[STEP 2] STANDARDIZATION (Calculating Z-score parameters)\n",
      "   >> Reference Statistics (calculated from Train):\n",
      "      - Mean Vector Sample (First 3): [ 0.8288  3.8002 10.0964]\n",
      "      - Std Vector Sample  (First 3): [0.1234 0.9449 6.7656]\n",
      "   >> Train Data Scaled.\n",
      "\n",
      "[STEP 3] PROCESSING TEST DATA\n",
      "   Input Shape: (2129,)\n",
      "   >> Applying stats learned from STEP 1...\n",
      "   >> Test Data Processed & Scaled.\n",
      "   Output Shape: (2129, 24)\n",
      "\n",
      "[STEP 4] CONSISTENCY CHECK\n",
      "   1. Column Count Match : [PASS]\n",
      "   2. NaN Count (Train)  : [0]\n",
      "   3. NaN Count (Test)   : [0]\n",
      "\n",
      ">> PIPELINE SUCCESS: Data is ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Process TRAIN Data (FIT & TRANSFORM) ---\n",
    "print(\"[STEP 1] PROCESSING TRAINING DATA\")\n",
    "print(f\"   Input Shape: {train_data.shape}\")\n",
    "\n",
    "# Thực hiện xử lý và HỌC các tham số (stats)\n",
    "X_train_processed, train_stats, feature_names = process_data_pipeline(train_data, is_train=True)\n",
    "\n",
    "print(f\"   >> Processing Complete.\")\n",
    "print(f\"   Output Shape: {X_train_processed.shape} ({len(feature_names)} features generated)\")\n",
    "print(f\"   >> LEARNED STATS (Evidence):\")\n",
    "print(f\"      - Median Experience (imputed): {train_stats['exp_median']} years\")\n",
    "print(f\"      - Gender Categories found    : {len(train_stats['cat_gender'])} {train_stats['cat_gender']}\")\n",
    "print(f\"      - Company Types found        : {len(train_stats['cat_company_type'])}\")\n",
    "\n",
    "\n",
    "# --- STEP 2: Standardization (Scaling) ---\n",
    "print(\"\\n[STEP 2] STANDARDIZATION (Calculating Z-score parameters)\")\n",
    "\n",
    "# Tính Mean và Std CHỈ trên tập Train\n",
    "mean_vals = np.mean(X_train_processed, axis=0)\n",
    "std_vals = np.std(X_train_processed, axis=0)\n",
    "\n",
    "# Xử lý trường hợp std = 0\n",
    "std_vals[std_vals == 0] = 1 \n",
    "\n",
    "print(f\"   >> Reference Statistics (calculated from Train):\")\n",
    "print(f\"      - Mean Vector Sample (First 3): {np.round(mean_vals[:3], 4)}\")\n",
    "print(f\"      - Std Vector Sample  (First 3): {np.round(std_vals[:3], 4)}\")\n",
    "\n",
    "# Áp dụng công thức\n",
    "X_train_scaled = (X_train_processed - mean_vals) / std_vals\n",
    "print(f\"   >> Train Data Scaled.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Process TEST Data (TRANSFORM ONLY) ---\n",
    "print(\"\\n[STEP 3] PROCESSING TEST DATA\")\n",
    "print(f\"   Input Shape: {test_data.shape}\")\n",
    "print(f\"   >> Applying stats learned from STEP 1...\")\n",
    "\n",
    "# Truyền 'train_stats' vào để áp dụng quy luật cũ\n",
    "X_test_processed, _, _ = process_data_pipeline(test_data, is_train=False, stats=train_stats)\n",
    "\n",
    "# Áp dụng Mean/Std CỦA TẬP TRAIN\n",
    "X_test_scaled = (X_test_processed - mean_vals) / std_vals\n",
    "\n",
    "print(f\"   >> Test Data Processed & Scaled.\")\n",
    "print(f\"   Output Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "\n",
    "# --- STEP 4: FINAL VALIDATION ---\n",
    "print(\"\\n[STEP 4] CONSISTENCY CHECK\")\n",
    "\n",
    "# Kiểm tra số cột\n",
    "cols_match = X_train_scaled.shape[1] == X_test_scaled.shape[1]\n",
    "print(f\"   1. Column Count Match : {'[PASS]' if cols_match else '[FAIL]'}\")\n",
    "\n",
    "# Kiểm tra NaN\n",
    "nan_train = np.isnan(X_train_scaled).sum()\n",
    "nan_test = np.isnan(X_test_scaled).sum()\n",
    "print(f\"   2. NaN Count (Train)  : {'[0]' if nan_train == 0 else f'[FAIL] {nan_train}'}\")\n",
    "print(f\"   3. NaN Count (Test)   : {'[0]' if nan_test == 0 else f'[FAIL] {nan_test}'}\")\n",
    "\n",
    "if cols_match and nan_train == 0 and nan_test == 0:\n",
    "    print(\"\\n>> PIPELINE SUCCESS: Data is ready for modeling.\")\n",
    "else:\n",
    "    print(\"\\n>> PIPELINE WARNING: Something needs to be fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99797e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV...\n",
      "Saved: ../data/processed/aug_train.csv (19158, 25)\n",
      "Saved: ../data/processed/aug_test.csv (2129, 24)\n",
      "\n",
      "Processing Pipeline Completed Successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. SAVE TO CSV (NumPy Only) ---\n",
    "\n",
    "# Tạo thư mục đầu ra\n",
    "output_dir = '../data/processed/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving to CSV...\")\n",
    "\n",
    "# --- A. XỬ LÝ TẬP TRAIN (aug_train.csv) ---\n",
    "# 1. Ghép Features và Target lại thành một ma trận duy nhất\n",
    "data_train_export = np.column_stack((X_train_scaled, y_train_raw.reshape(-1, 1)))\n",
    "\n",
    "# 2. Tạo Header (Danh sách tên cột + 'target')\n",
    "header_train = \",\".join(feature_names + ['target'])\n",
    "\n",
    "# 3. Lưu file\n",
    "train_file_path = os.path.join(output_dir, 'aug_train.csv')\n",
    "np.savetxt(\n",
    "    train_file_path,        \n",
    "    data_train_export,        \n",
    "    delimiter=\",\",            \n",
    "    header=header_train,      \n",
    "    comments='',              \n",
    "    fmt='%.6f'                \n",
    ")\n",
    "print(f\"Saved: {train_file_path} {data_train_export.shape}\")\n",
    "\n",
    "\n",
    "# --- B. XỬ LÝ TẬP TEST (aug_test.csv) ---\n",
    "header_test = \",\".join(feature_names)\n",
    "\n",
    "test_file_path = os.path.join(output_dir, 'aug_test.csv')\n",
    "np.savetxt(\n",
    "    test_file_path,\n",
    "    X_test_scaled,\n",
    "    delimiter=\",\",\n",
    "    header=header_test,\n",
    "    comments='',\n",
    "    fmt='%.6f'\n",
    ")\n",
    "print(f\"Saved: {test_file_path} {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\nProcessing Pipeline Completed Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sanity Check ---\n",
      "Any NaN in Train? 0\n",
      "Any NaN in Test?  0\n",
      "Column count matches. Pipeline is robust.\n"
     ]
    }
   ],
   "source": [
    "# Quick Sanity Check\n",
    "print(\"--- Sanity Check ---\")\n",
    "print(f\"Any NaN in Train: {np.isnan(X_train_scaled).sum()}\")\n",
    "print(f\"Any NaN in Test:  {np.isnan(X_test_scaled).sum()}\")\n",
    "\n",
    "# Kiểm tra xem số cột có khớp nhau không (Quan trọng!)\n",
    "assert X_train_scaled.shape[1] == X_test_scaled.shape[1], \"CRITICAL ERROR: Train and Test features mismatch!\"\n",
    "print(\"Column count matches. Pipeline is robust.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
